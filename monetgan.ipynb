{"cells":[{"metadata":{},"cell_type":"markdown","source":"1. Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -fr *.png\n!rm -fr *.gif\n!rm -fr *.mp4\nimport shutil\nshutil.rmtree(\"./training_checkpoints\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary libraries\n\nfrom os import listdir\nfrom matplotlib import image\nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport os\nimport time\nimport glob\nimport imageio\nimport PIL\nfrom IPython import display\nfrom keras.preprocessing.image import img_to_array\nfrom random import randint\nimport numpy as np\n\ntry:\n    tpu = tf.distribute.cluster_resolver.GPUClusterResolver()\n    print('Device:', gpu.master())\n    tf.config.experimental_connect_to_cluster(gpu)\n    tf.gpu.experimental.initialize_gpu_system(gpu)\n    strategy = tf.distribute.experimental.GPUStrategy(gpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n    \nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for loading data\n\ndef load_dataset(image_dir):\n    '''\n    loads all images from directory\n    Input: image_dir = file path for training images\n    Output: dataset = images stored as array\n    '''\n    dataset = []\n    for filename in listdir(image_dir):\n        img_data = cv2.imread(image_dir + filename) # load image\n        dataset.append(img_data) # store loaded image\n    \n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load data\n\nimage_dir = '/kaggle/input/gan-getting-started/monet_jpg/'\ndata = load_dataset(image_dir)\nprint(len(data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.array(data)\ndata = data.astype('float32')\ndata = (data / 127.5) - 1 #normalise values to [-1, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a look at an example\n\nexample = data[randint(0, 300)]\n# summarize shape of the pixel array\nprint(example.dtype)\nprint(example.shape)\n# display the array of pixels as an image\nplt.imshow(example)\nplt.show()\n\nexample_corr = (example + 1) * 127.5\nexample_corr = example_corr.astype(int)\nplt.imshow(example_corr)\nplt.show()\nprint('example', example.min(), example.max(), example.mean(), example.std())\nprint('example_corrected', example_corr.min(), example_corr.max(), example_corr.mean(), example_corr.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = np.mean(data, 0)\nplt.imshow(mean)\nplt.show()\n\nstd_dev = np.std(data, 0)\nplt.imshow(std_dev)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BUFFER_SIZE = 8000\nBATCH_SIZE = 32\n\n# Batch and shuffle the data\ntrain_dataset = tf.data.Dataset.from_tensor_slices(data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Model Definitions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make the generator\n# Code adapted from Tensorflow dcgan tutorial\n\ndef make_generator():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(8*8*128, use_bias=False, input_shape=(256,)))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((8, 8, 128)))\n    assert model.output_shape == (None, 8, 8, 128) # Note: None is the batch size\n\n    model.add(layers.Conv2DTranspose(1024, (4, 4), strides=(1, 1), padding='same', use_bias=False))\n    assert model.output_shape == (None, 8, 8, 1024)\n    model.add(layers.Dropout(0.5))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(512, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 16, 16, 512)\n    model.add(layers.Dropout(0.5))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Conv2DTranspose(512, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 32, 32, 512)\n    model.add(layers.Dropout(0.3))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Conv2DTranspose(512, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 64, 64, 512)\n    model.add(layers.Dropout(0.3))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Conv2DTranspose(256, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 128, 128, 256)\n    model.add(layers.Dropout(0.3))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Conv2DTranspose(3, (4, 4), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    assert model.output_shape == (None, 256, 256, 3)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generator sanity check\n\ngenerator = make_generator()\n\nnoise = tf.random.normal([1, 256])\ngenerated_image = generator(noise, training=False)\n\nplt.imshow(generated_image[0, :, :, 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make the discriminator\n# Code adapted from Tensorflow dcgan tutorial\n\ndef make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same',\n                                     input_shape=[256, 256, 3]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(256, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(512, (4, 4), strides=(1, 1), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Discriminator sanity check\n\ndiscriminator = make_discriminator_model()\ndecision = discriminator(generated_image)\nprint (decision)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loss functions\n\n# This method returns a helper function to compute cross entropy loss\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optimisers\n\ngenerator_optimizer = tf.keras.optimizers.Adam(lr = 1e-4, beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(lr = 1e-5, beta_1=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checkpoints\n\ncheckpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 50\nnoise_dim = 256\nnum_examples_to_generate = 1\n\n# We will reuse this seed overtime (so it's easier)\n# to visualize progress in the animated GIF)\nseed = tf.random.normal([num_examples_to_generate, noise_dim])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Notice the use of `tf.function`\n# This annotation causes the function to be \"compiled\".\n@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      generated_images = generator(noise, training=True)\n\n      real_output = discriminator(images, training=True)\n      fake_output = discriminator(generated_images, training=True)\n\n      gen_loss = generator_loss(fake_output)\n      disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n    \n    return gen_loss, disc_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(dataset, epochs):\n  for epoch in range(epochs):\n    start = time.time()\n\n    for image_batch in dataset:\n      gen_loss, disc_loss = train_step(image_batch)\n\n    # Produce images for the GIF as we go\n    display.clear_output(wait=True)\n    generate_and_save_images(generator,\n                             epoch + 1,\n                             seed)\n\n    # Save the model every 15 epochs\n    if (epoch + 1) % 15 == 0:\n      checkpoint.save(file_prefix = checkpoint_prefix)\n\n    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n    print ('gen loss = {}, disc loss = {}'.format(gen_loss, disc_loss))\n\n  # Generate after the final epoch\n  display.clear_output(wait=True)\n  generate_and_save_images(generator,\n                           epochs,\n                           seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_and_save_images(model, epoch, test_input):\n  # Notice `training` is set to False.\n  # This is so all layers run in inference mode (batchnorm).\n    predictions = model(test_input, training=False)\n    predictions = (predictions + 1) * 127.5\n    predictions = tf.dtypes.cast(predictions, tf.uint8)\n    fig = plt.figure(figsize=(8, 8))\n\n    for i in range(predictions.shape[0]):\n     plt.subplot(1, 1, i+1)\n     plt.imshow(predictions[i, :, :, :])\n     plt.axis('off')\n     #predictions = (predictions + 1) * 127.5\n     #predictions = tf.dtypes.cast(predictions, tf.uint8)\n     #plt.imshow((predictions[0]))\n     #plt.axis('off')\n    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch), bbox_inches='tight')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(train_dataset, EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Model Output"},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n\n# Display a single image using the epoch number\ndef display_image(epoch_no):\n  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n\ndisplay_image(EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anim_file = 'dcgan.gif'\n\nwith imageio.get_writer(anim_file, mode='I') as writer:\n  filenames = glob.glob('image*.png')\n  filenames = sorted(filenames)\n  for filename in filenames:\n    image = imageio.imread(filename)\n    writer.append_data(image)\n  image = imageio.imread(filename)\n  writer.append_data(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To generate GIFs\n!pip install git+https://github.com/tensorflow/docs\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow_docs.vis.embed as embed\nembed.embed_file(anim_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install imageio-ffmpeg\nimport imageio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython import display as ipythondisplay\nimport io\nimport os\nimport base64\nfrom IPython.display import HTML\n\ndef show_video(vid):\n  #mp4list = [video\n  #if len(mp4list) > 0:\n  ext = os.path.splitext(vid)[-1][1:]\n  video = io.open(vid, 'r+b').read()\n  #encoded = base64.b64encode(video)\n  ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n              loop controls style=\"height: 400px;\">\n              <source src=\"data:video/{1}';base64,{0}\" type=\"video/{1}\" />\n              </video>'''.format(base64.b64encode(video).decode('ascii'), ext)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z1 = tf.random.normal([1, noise_dim], stddev=2.0)\nz2 = tf.random.normal([1, noise_dim], stddev=2.0)\n\nimage = generator(z1, training=False)\nimage = (image + 1) * 127.5\nimage = tf.dtypes.cast(image, tf.uint8)\nplt.imshow(image[0, :, :, :])\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z1 = tf.random.normal([1, noise_dim], stddev=4)\nz2 = tf.random.normal([1, noise_dim], stddev=4)\n\n\nnumber_of_steps = 100\n\ndef generate_images(zs):       \n    imgs = []\n    count = 0\n    for z in zs:\n        count +=1\n        images = generator(z)\n        images = (images + 1) * 127.5\n        images = tf.dtypes.cast(images, tf.uint8)\n        plt.imshow((images[0]))\n        plt.axis('off')\n        plt.savefig('image_at_z_{:04d}.png'.format(count), bbox_inches='tight')\n        imgs.append(images)\n    return imgs\n\ndef interpolate(zs, steps):\n   out = []\n   for i in range(len(zs)-1):\n    for index in range(steps):\n     fraction = index/float(steps) \n     out.append(zs[i+1]*fraction + zs[i]*(1-fraction))\n   return out\n\nimgs = generate_images(interpolate([z1,z2],number_of_steps))\n\n# Example of reading a generated set of images, and storing as MP4.\nmovieName = 'mov.mp4'\n\nwith imageio.get_writer(movieName, mode='I') as writer:\n  filenames = glob.glob('image_at_z*.png')\n  filenames = sorted(filenames)\n  for filename in filenames:\n    image = imageio.imread(filename)\n    writer.append_data(image)\n  image = imageio.imread(filename)\n  writer.append_data(image)\nshow_video(movieName)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(z1, z2)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}